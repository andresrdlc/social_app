{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fileUrl, spark):\n",
    "    df = spark.read.csv(fileUrl, sep=\",\", inferSchema=True, header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    df = df.withColumnRenamed('_c0', \"id\").withColumnRenamed('_c1', 'label').withColumnRenamed('_c2', 'tweet')\n",
    "    \n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', '[^a-z0-9A-Z`~!@#$%&<>?., ]', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', '[0-9`~!@#$%&<>?,\\']', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', 'http://*.*.com', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', 'www.*.com', ''))\n",
    "    df = df.withColumn('tweet', regexp_replace('tweet', '\\.', ''))\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "    wordData = tokenizer.transform(df)\n",
    "    \n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"word_clean\")\n",
    "    word_clean_data = remover.transform(wordData)\n",
    "    \n",
    "    count = CountVectorizer(inputCol=\"word_clean\", outputCol=\"rawFeatures\")\n",
    "    model = count.fit(word_clean_data)\n",
    "    \n",
    "    featurizedData = model.transform(word_clean_data)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    \n",
    "    return rescaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    seed = 0\n",
    "    trainDf, testDf = df.randomSplit([0.7, 0.3], seed)\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_table(train_predictions, test_predictions):\n",
    "    train_predictions.groupBy('label', 'prediction').count().show()\n",
    "    test_predictions.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, labelCol=\"label\", predictionCol=\"prediction\"):\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=predictionCol, labelCol=labelCol, metricName=\"areaUnderROC\")\n",
    "    roc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return {\"ROC\": roc, \"F1\": f1, \"Accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_data, test_data):\n",
    "    lr = LogisticRegression(maxIter=15)\n",
    "    paramGrid_lr = ParamGridBuilder().build()\n",
    "    \n",
    "    crossval_lr = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lr, evaluator=BinaryClassificationEvaluator(), numFolds=8)\n",
    "    cvModel_lr = crossval_lr.fit(train_data)\n",
    "    \n",
    "    best_model_lr = cvModel_lr.bestModel\n",
    "    train_fit_lr = best_model_lr.transform(train_data)\n",
    "    train_summary = evaluate_model(train_fit_lr)\n",
    "    \n",
    "    predictions_lr = cvModel_lr.transform(test_data)\n",
    "    test_summary = evaluate_model(predictions_lr)\n",
    "\n",
    "    details_table(train_fit_lr, predictions_lr)\n",
    "    \n",
    "    return train_summary, test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tweet` cannot be resolved. Did you mean one of the following? [`id`].;\n'Project [id#393, regexp_replace('tweet, [^a-z0-9A-Z`~!@#$%&<>?., ], , 1) AS tweet#398]\n+- Project [_c0#391 AS id#393]\n   +- Relation [_c0#391] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Causario\\Desktop\\RedSocial\\social_app\\data\\sentiment_analysis\\Testing\\sa_lr_pyspark_test.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m read_file(url, spark)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(df)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m pre_process(df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_data, test_data \u001b[39m=\u001b[39m train_test_split(df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_summary, test_summary \u001b[39m=\u001b[39m logistic_regression(train_data, test_data)\n",
      "\u001b[1;32mc:\\Users\\Causario\\Desktop\\RedSocial\\social_app\\data\\sentiment_analysis\\Testing\\sa_lr_pyspark_test.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_process\u001b[39m(df):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumnRenamed(\u001b[39m'\u001b[39m\u001b[39m_c0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mwithColumnRenamed(\u001b[39m'\u001b[39m\u001b[39m_c1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mwithColumnRenamed(\u001b[39m'\u001b[39m\u001b[39m_c2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mwithColumn(\u001b[39m'\u001b[39;49m\u001b[39mtweet\u001b[39;49m\u001b[39m'\u001b[39;49m, regexp_replace(\u001b[39m'\u001b[39;49m\u001b[39mtweet\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m[^a-z0-9A-Z`~!@#$\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m&<>?., ]\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m, regexp_replace(\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m[0-9`~!@#$\u001b[39m\u001b[39m%\u001b[39m\u001b[39m&<>?,\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/Testing/sa_lr_pyspark_test.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m, regexp_replace(\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp://*.*.com\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Causario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5166\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5167\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5168\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m   5169\u001b[0m     )\n\u001b[1;32m-> 5170\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\Causario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Causario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tweet` cannot be resolved. Did you mean one of the following? [`id`].;\n'Project [id#393, regexp_replace('tweet, [^a-z0-9A-Z`~!@#$%&<>?., ], , 1) AS tweet#398]\n+- Project [_c0#391 AS id#393]\n   +- Relation [_c0#391] csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"SocialApp\").getOrCreate()\n",
    "    url = \"C:/Users/Causario/Desktop/RedSocial/social_app/data/sentiment_analysis/prueba.txt\"  # Provide the path to file\n",
    "    df = read_file(url, spark)\n",
    "    df = pre_process(df)\n",
    "    train_data, test_data = train_test_split(df)\n",
    "    train_summary, test_summary = logistic_regression(train_data, test_data)\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
